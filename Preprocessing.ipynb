{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yklj6M8AlY0Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1SFQJ7mkU8z"
      },
      "outputs": [],
      "source": [
        "# 3 helper func parse_probe, process_parquet and process_batch\n",
        "def parse_probe(path):\n",
        "  \"\"\"\n",
        "  Parse probe.txt\n",
        "\n",
        "  Returns (movie_id, user_id) pairs\n",
        "  \"\"\"\n",
        "  probe_dict = {}\n",
        "  curr_movie = None\n",
        "\n",
        "  with open(path, 'r') as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      if line.endswith(':'):\n",
        "        curr_movie = int(line[:-1])\n",
        "      elif line and curr_movie is not None:\n",
        "        probe_dict[(curr_movie, int(line))] = None\n",
        "\n",
        "  return probe_dict\n",
        "\n",
        "def process_batch(batch, probe_pairs, output, append=False):\n",
        "  \"\"\"\n",
        "  Process batch of data. Updating probe_pairs\n",
        "\n",
        "  Args\n",
        "    batch: df batch to process\n",
        "    probe_pairs: (movie_id, user_id) pairs\n",
        "    output: path to save processed data\n",
        "    append: whether to append to existing file\n",
        "\n",
        "  Returns\n",
        "    tuple: (train_count, probe_count)\n",
        "  \"\"\"\n",
        "\n",
        "  # probe key now matches up to a column in the df batch: 'temp_key'\n",
        "  batch['temp_key'] = batch['movie_id'].astype(str) + '_' + batch['user_id'].astype(str)\n",
        "\n",
        "  probe_keys = {f\"{movie_id}_{user_id}\" for movie_id, user_id in probe_pairs.keys()}\n",
        "\n",
        "  # mask to separate probe and train data\n",
        "  mask = batch['temp_key'].isin(probe_keys)\n",
        "\n",
        "  probe_rows = batch[mask]\n",
        "  probe_count = len(probe_rows)\n",
        "\n",
        "  # assign rating to movie_id, user_id pairs\n",
        "  if probe_count > 0:\n",
        "    for _, row in probe_rows.iterrows():\n",
        "      movie_id = row['movie_id']\n",
        "      user_id = row['user_id']\n",
        "      probe_pairs[(movie_id, user_id)] = (row['rating'], row['date'])\n",
        "\n",
        "  # filter out training rows using mask\n",
        "  train_batch = batch[~mask].drop(columns=['temp_key']).reset_index(drop=True)\n",
        "  train_count = len(train_batch)\n",
        "\n",
        "  # saving training rows to parquet\n",
        "  if train_count > 0:\n",
        "    table = pa.Table.from_pandas(train_batch, preserve_index=False)\n",
        "\n",
        "    if os.path.exists(output):\n",
        "\n",
        "      # Append if pq file already exists\n",
        "      with pq.ParquetWriter(output,\n",
        "        table.schema,\n",
        "        compression='zstd',\n",
        "      ) as writer:\n",
        "        writer.write_table(table)\n",
        "\n",
        "    else:\n",
        "      # otherwise make new with compression\n",
        "      train_batch.to_parquet(output, compression='zstd', engine='pyarrow',index=False)\n",
        "\n",
        "  return (train_count, probe_count)\n",
        "\n",
        "def process_parquet(df, probe_pairs, output_base, batch_size=3):\n",
        "  \"\"\"\n",
        "  Process parquet file. Get grouth truth ratings for each probe pair\n",
        "  Remove probe datapoints from training data\n",
        "\n",
        "  Args\n",
        "    df: path to Parquet file\n",
        "    probe_pairs: (movie_id, user_id) pairs\n",
        "    output: path to save processed data\n",
        "    batch_size: number of pq ROW GROUPS to process at a time\n",
        "\n",
        "  Returns\n",
        "    tuple: (training_count, probe_count)\n",
        "  \"\"\"\n",
        "  probe_count = 0\n",
        "  train_count = 0\n",
        "  output_files = []\n",
        "\n",
        "  #batch_size = 500000\n",
        "  #first_batch = True\n",
        "\n",
        "  if not isinstance(df, str):\n",
        "    raise TypeError(\"df must be path to a Parquet file\")\n",
        "\n",
        "  pq_file = pq.ParquetFile(df)\n",
        "  total_row_groups = pq_file.num_row_groups\n",
        "\n",
        "  for group in tqdm(range(0, total_row_groups, batch_size)):\n",
        "    #print(f'Processing row group {group+1}/{total_row}')\n",
        "\n",
        "    row_groups_batch = list(range(group, min(group + batch_size, total_row_groups)))\n",
        "    print(f\"Processing rowgroups {row_groups_batch}\")\n",
        "\n",
        "    batch = pd.concat([pq_file.read_row_group(i).to_pandas() for i in row_groups_batch], ignore_index=True)\n",
        "\n",
        "    min_movie_id = batch['movie_id'].min()\n",
        "    max_movie_id = batch['movie_id'].max()\n",
        "\n",
        "    # build file name to describe partition's content\n",
        "    output_path = f\"{output_base}_group_{group//batch_size}_movies_{min_movie_id}-{max_movie_id}.parquet\"\n",
        "    output_files.append(output_path)\n",
        "\n",
        "    batch_train_count, batch_probe_count = process_batch(batch, probe_pairs, output_path,\n",
        "                                                          append=False) # append = not first_batch\n",
        "\n",
        "    train_count += batch_train_count\n",
        "    probe_count += batch_probe_count\n",
        "    #first_batch = False\n",
        "\n",
        "  return train_count, probe_count, output_files\n",
        "\n",
        "def create_test_df(probe_pairs, output):\n",
        "  \"\"\"\n",
        "  Create test dataframe from probe_pairs\n",
        "\n",
        "  Args\n",
        "    probe_pairs: dict with (movie_id, user_id) as key and rating as value\n",
        "    output: path to save processed data\n",
        "\n",
        "  Returns\n",
        "    test_df: probe entries + rating\n",
        "  \"\"\"\n",
        "\n",
        "  test_data = []\n",
        "  none_count = 0\n",
        "\n",
        "  print(f'Current probe pair sample {probe_pairs.keys()}')\n",
        "  print(f'Current probe pair sample {probe_pairs.values()}')\n",
        "\n",
        "  for (movie_id, user_id), data in probe_pairs.items():\n",
        "    if data is not None:\n",
        "      test_data.append({'movie_id': movie_id,\n",
        "                        'user_id': user_id,\n",
        "                        'rating': data[0],\n",
        "                        'date': data[1]})\n",
        "    else:\n",
        "      none_count += 1\n",
        "\n",
        "  test_df = pd.DataFrame(test_data)\n",
        "  print(f\"Created test set with {len(test_df):,} entries\")\n",
        "\n",
        "  test_df.to_parquet(output, compression='zstd', engine='pyarrow')\n",
        "\n",
        "  return none_count\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_netflix(dfs, probe_path, save_dir):\n",
        "  \"\"\"\n",
        "  Process netflix data. Get grouth truth ratings for each probe pair\n",
        "  Remove probe datapoints from training data\n",
        "\n",
        "  Args\n",
        "    dfs: list of dataframes of Netflix data\n",
        "    probe_path: path to probe.txt\n",
        "    save_dir: directory to save processed data\n",
        "\n",
        "  Returns\n",
        "    train_df: dataframes with probe entries removed\n",
        "    test_df: probe entries + rating\n",
        "  \"\"\"\n",
        "\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  # Parse probe into dictionary > faster lookup\n",
        "  print(\"Parsing probe file\")\n",
        "  probe_dict = parse_probe(probe_path)\n",
        "  print(f\"Found {len(probe_dict)} probe pairs\")\n",
        "\n",
        "\n",
        "  # Extract ratings from probe entries and remove from training set\n",
        "  train_files = []\n",
        "\n",
        "  total_train = 0\n",
        "  total_matches = 0\n",
        "\n",
        "  for i, df_source in tqdm(enumerate(dfs)):\n",
        "\n",
        "    # construct save path -> saving to multiple files\n",
        "    train_path_base = os.path.join(save_dir, f'train_part_{i+1}')\n",
        "\n",
        "    part_train_count, part_probe_count, df_files = process_parquet(\n",
        "        df_source, probe_dict, train_path_base\n",
        "    )\n",
        "\n",
        "    total_train += part_train_count\n",
        "    total_matches += part_probe_count\n",
        "    train_files.extend(df_files)\n",
        "\n",
        "    print(f\"Source {i+1} stats:\")\n",
        "    print(f\"  - Added {part_train_count:,} ratings to training set\")\n",
        "    print(f\"  - Found {part_probe_count:,} probe ratings\")\n",
        "\n",
        "  # Create test set from probe dict\n",
        "  test_path = os.path.join(save_dir, 'test.parquet')\n",
        "  none_count = create_test_df(probe_dict, test_path)\n",
        "  print(f\"Created test set with {none_count:,} missing ratings\")\n",
        "\n",
        "  print(\"\\nProcessing complete!\")\n",
        "  print(f\"Total training ratings: {total_train:,}\")\n",
        "  print(f\"Total probe ratings found: {total_matches:,} out of {len(probe_dict):,}\")\n",
        "\n",
        "  return train_files, test_path"
      ],
      "metadata": {
        "id": "dnEDoB7gWC2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESSING\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "working_dir = '/gdrive/MyDrive/Netflix_Prize'\n",
        "os.chdir(working_dir)\n",
        "\n",
        "dfs = [\n",
        "    'combined_data_1.parquet',\n",
        "    'combined_data_2.parquet',\n",
        "    'combined_data_3.parquet',\n",
        "    'combined_data_4.parquet',\n",
        "]\n",
        "\n",
        "probe_path = 'probe.txt'\n",
        "save_dir = 'processed'"
      ],
      "metadata": {
        "id": "G6GewH0Fhx0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process files\n",
        "\n",
        "train_files, test_file = process_netflix(dfs, probe_path, save_dir)\n",
        "\n",
        "print(\"Complete\")\n",
        "print(f\"Train files: {train_files}\")\n",
        "print(f\"Test file: {test_file}\")"
      ],
      "metadata": {
        "id": "4qgxlPqjhh3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_parquet('processed/test.parquet')"
      ],
      "metadata": {
        "id": "1puHmUxbGGML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_parquet('processed/train_part_1_group_0_movies_1-571.parquet')"
      ],
      "metadata": {
        "id": "XUei3i5GTVNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9Yy5ewecSha"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy995fPpxSvDK8P3u/cf7T"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}